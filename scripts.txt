# Import the json to neo4j graph

// --- 0. (No explicit constraint drops from within this script) ---
// Based on your 'SHOW CONSTRAINTS' output, the problematic `:Quote(text)` constraint is gone.
// If the "Property value is too large to index" error reappears, it implies a
// constraint on `:Quote(text)` has been re-added, and you would need to manually
// drop it again (e.g., DROP CONSTRAINT constraint_name;).


// --- 1. OPTIONAL: Clear Existing Data (Use with EXTREME CAUTION) ---
// If this is a brand new Neo4j instance or you want to wipe everything
// before importing, uncomment the line below.
// WARNING: This will delete ALL nodes and relationships in your graph!
// MATCH (n) DETACH DELETE n;


// --- 2. Create Uniqueness Constraints for Efficiency and Data Integrity ---
// These constraints ensure data integrity and optimize MERGE operations.
// We are re-adding the constraint on 'hash' for Quote nodes for performance.
CREATE CONSTRAINT IF NOT EXISTS FOR (a:Author) REQUIRE a.name IS UNIQUE;
CREATE CONSTRAINT IF NOT EXISTS FOR (q:Quote) REQUIRE q.hash IS UNIQUE; // RE-ADDED for fast MERGE on Quote nodes
CREATE CONSTRAINT IF NOT EXISTS FOR (c:Context) REQUIRE c.text IS UNIQUE; // Using 'text' property as per Python
CREATE CONSTRAINT IF NOT EXISTS FOR (d:Detail) REQUIRE d.text IS UNIQUE;


// --- 3. Load the JSON Lines Data and Process Each Object IN BATCHES ---
// apoc.periodic.iterate processes the data in smaller transactions (batches)
// to avoid memory exhaustion for large imports.
CALL apoc.periodic.iterate(
    "CALL apoc.load.json('file:///quotes.json') YIELD value", // Outer query: loads data and yields individual JSON objects
    "
    // Inner query: processes each 'value' (JSON object) from the outer query
    WITH value AS quoteData

    // 3.1 Create or Merge the Author Node
    MERGE (a:Author {name: quoteData.author})

    // 3.2 Generate a unique hash for the quote text (for fast MERGE lookup)
    // Then create or Merge the Quote Node using its hash.
    // The full original quote text is set as the 'text' property.
    WITH quoteData, a, apoc.util.sha256([quoteData.quote]) AS quoteHash
    MERGE (q:Quote {hash: quoteHash})
    ON CREATE SET q.text = quoteData.quote // Store the full original quote text
    // ON MATCH SET q.text = quoteData.quote // Optional: Uncomment if you want to ensure text is updated on re-runs

    // 3.3 Link Author to Quote
    MERGE (a)-[:WROTE]->(q) // Relationship type changed to :WROTE as per Python

    // 3.4 Conditionally Create/Merge the Context Node and Relationship
    // Uses 'text' property for Context, matching Python.
    WITH quoteData, q
    WHERE quoteData.context IS NOT NULL
    MERGE (c:Context {text: quoteData.context}) // Using 'text' property as per Python
    MERGE (q)-[:HAS_CONTEXT]->(c)

    // 3.5 Iterate through 'sources' (from JSON) to create 'Detail' Nodes and Relationships
    FOREACH (sourceData IN quoteData.sources |
        // Create or Merge the Detail Node (using the ':Detail' label and 'text' property)
        MERGE (d:Detail {text: sourceData.text})

        // Create the HAS_DETAIL Relationship from the Quote to the Detail
        MERGE (q)-[:HAS_DETAIL]->(d)
    )
    ",
    // Configuration for apoc.periodic.iterate:
    // batchSize: Number of items processed per transaction (adjust as needed)
    // parallel: false for safer initial import, true for potentially faster but more resource-intensive
    // iterateList: true because apoc.load.json yields individual items from a list
    {batchSize: 1000, parallel: false, iterateList: true} // You can adjust batchSize, 1000-10000 is usually good
) YIELD batches, total
RETURN batches, total; // Returns the number of batches processed and total items

# Is graph working query check
MATCH (a:Author {name: "Albert Einstein"})-[:WROTE]->(q:Quote)
OPTIONAL MATCH (q)-[:HAS_CONTEXT]->(c:Context)
OPTIONAL MATCH (q)-[:HAS_DETAIL]->(d:Detail)
RETURN
    q.text AS Quote,
    a.name AS Author,
    c.text AS Context, // Using 'text' property for Context as per the latest script
    collect(DISTINCT d.text) AS Sources // Collecting all unique source details
ORDER BY q.text // Optional: Order the results by quote text for readability